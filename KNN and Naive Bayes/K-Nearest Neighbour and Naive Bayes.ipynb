{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is k-Nearest Neighbors\n",
    "\n",
    "The model for kNN is the entire training dataset. When a prediction is required for a unseen data instance, the kNN algorithm will search through the training dataset for the k-most similar instances. The prediction attribute of the most similar instances is summarized and returned as the prediction for the unseen instance.\n",
    "\n",
    "The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance can be used. Other other types of data such as categorical or binary data, Hamming distance can be used.\n",
    "\n",
    "In the case of regression problems, the average of the predicted attribute may be returned. In the case of classification, the most prevalent class may be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does k-Nearest Neighbors Work\n",
    "\n",
    "The kNN algorithm is belongs to the family of instance-based, competitive learning and lazy learning algorithms.\n",
    "\n",
    "Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model.\n",
    "\n",
    "It is a competitive learning algorithm, because it internally uses competition between model elements (data instances) in order to make a predictive decision. The objective similarity measure between data instances causes each data instance to compete to “win” or be most similar to a given unseen data instance and contribute to a prediction.\n",
    "\n",
    "Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets.\n",
    "\n",
    "Finally, kNN is powerful because it does not assume anything about the data, other than a distance measure can be calculated consistently between any two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn classifier implementation in scikit learn\n",
    "\n",
    "We are going to examine the Breast Cancer Dataset using python sklearn library to model K-nearest neighbor algorithm. After modeling the knn classifier, we are going to use the trained knn model to predict whether the patient is suffering from the benign tumor or malignant tumor. The greatness of using  Sklearn is that it provides us the functionality to implement machine learning algorithms in a few lines of code.\n",
    "\n",
    "\n",
    "As we discussed the principle behind KNN classifier (K-Nearest Neighbor) algorithm is to find K predefined number of training samples closest in the distance to new point & predict the label from these. The distance measure is commonly considered to be Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance\n",
    "\n",
    "Euclidean distance is the most commonly used distance measure. Euclidean distance also called as simply distance. The usage of Euclidean distance measure is highly recommended when data is dense or continuous. Euclidean distance is the best proximity measure. The Euclidean distance between two points is the length of the path connecting them.The Pythagorean theorem gives this distance between two points.\n",
    "\n",
    "### Euclidean distance implementation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.746794344808963"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import*\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    " \n",
    "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    " \n",
    "euclidean_distance([0,3,4,5],[7,6,3,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisconsin Breast Cancer Data Set\n",
    "\n",
    "The Wisconsin Breast Cancer Database was collected by Dr. William H. Wolberg (physician), University of Wisconsin Hospitals, USA. This dataset consists of 10 continuous attributes and 1 target class attributes. Class attribute shows the observation result, whether the patient is suffering from the benign tumor or malignant tumor. Benign tumors do not spread to other parts while the malignant tumor is cancerous. The dataset was collected & openly distributed so as to find out some patterns from this data.\n",
    "\n",
    "Class attribute shows the observation result, whether the patient is suffering from the benign tumor or malignant tumor. Benign tumors do not spread to other parts while the malignant tumor is cancerous. The dataset was collected & openly distributed so as to find out some patterns from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data Set Attribute Information:\n",
    "\n",
    "1. Sample code number: id number\n",
    "2. Clump Thickness: 1 – 10\n",
    "3. Uniformity of Cell Size: 1 – 10\n",
    "4. Uniformity of Cell Shape: 1 – 10\n",
    "5. Marginal Adhesion: 1 – 10\n",
    "6. Single Epithelial Cell Size: 1 – 10\n",
    "7. Bare Nuclei: 1 – 10\n",
    "8. Bland Chromatin: 1 – 10\n",
    "9. Normal Nucleoli: 1 – 10\n",
    "10. Mitoses: 1 – 10\n",
    "11. Class: (2 for benign, 4 for malignant)\n",
    "\n",
    "#### Problem Statement:\n",
    "\n",
    "To model the knn classifier using the Breast Cancer data for predicting whether a patient is suffering from the benign tumor or malignant tumor.\n",
    "\n",
    "#### KNN Model for Cancerous tumor detection:\n",
    "\n",
    "To diagnose Breast Cancer, the doctor uses his experience by analyzing details provided by\n",
    "\n",
    "   * Patient’s Past Medical History\n",
    "   * Reports of all the tests performed.\n",
    "\n",
    "\n",
    "Using the modeled KNN classifier, we will solve the problem in a way similar to the procedure used by doctors. The modeled KNN classifier will compare the new patient’s test reports, observation metrics with the records of patients(training data) that correctly classified as benign or malignant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import:\n",
    "\n",
    "We are using breast cancer data. You can download it from archive.ics.uci.edu website. For importing the data and manipulating it, we are going to use numpy arrays.\n",
    "Using genfromtxt() method, we are importing our dataset into the 2d numpy array. You can import text files using this function. We are passing 3 parameters:\n",
    "\n",
    "fname\n",
    "It handles the filename with extension.\n",
    "\n",
    "\n",
    "delimiter\n",
    " The string used to separate values. In our dataset “,”(comma) is the separator.\n",
    "\n",
    "\n",
    "dtype\n",
    "It handles data type of variables.\n",
    "\n",
    "\n",
    "All the values are numeric in our database. But some values are missing and are replaced by “?”. So, we will have to perform data imputation. Due to this reason, we are using float dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer_data = np.genfromtxt(\n",
    " fname ='breast-cancer-wisconsin.data', delimiter= ',', dtype= float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght::  699\n",
      "Dataset::  [[  1.00002500e+06   5.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
      "    1.00000000e+00   2.00000000e+00]\n",
      " [  1.00294500e+06   5.00000000e+00   4.00000000e+00 ...,   2.00000000e+00\n",
      "    1.00000000e+00   2.00000000e+00]\n",
      " [  1.01542500e+06   3.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
      "    1.00000000e+00   2.00000000e+00]\n",
      " ..., \n",
      " [  8.88820000e+05   5.00000000e+00   1.00000000e+01 ...,   1.00000000e+01\n",
      "    2.00000000e+00   4.00000000e+00]\n",
      " [  8.97471000e+05   4.00000000e+00   8.00000000e+00 ...,   6.00000000e+00\n",
      "    1.00000000e+00   4.00000000e+00]\n",
      " [  8.97471000e+05   4.00000000e+00   8.00000000e+00 ...,   4.00000000e+00\n",
      "    1.00000000e+00   4.00000000e+00]]\n",
      "Dataset Shape::  (699, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Lenght:: \", len(cancer_data))\n",
    "print(\"Dataset:: \", str(cancer_data))\n",
    "print(\"Dataset Shape:: \", cancer_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cancer dataset’s first column consists of patient’s id. To make this prediction process unbiased, we should remove this patient id. We can use numpy delete() method for this operation.\n",
    "\n",
    "delete(): It returns a new transformed array. Three parameters should to passed.\n",
    "\n",
    "   * arr: It holds the array name.\n",
    "   * obj: It indicates which sub-arrays to remove.\n",
    "   * axis: The axis along which to delete. axis = 1 is used for columns & axis = 0 for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer_data = np.delete(arr = cancer_data, obj= 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wish to divide the dataset into feature & label dataset. i.e., feature data is predictor variables they will help us to predict labels(criterion variable). Here, first 9 columns include continuous variables that will help us to predict whether a patient is having the benign tumor or malignant tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = cancer_data[:,range(0,9)]\n",
    "Y = cancer_data[:,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation:\n",
    "\n",
    "Imputation is a process of replacing missing values with substituted values. In our dataset, some columns have missing values. We can replace missing values with mean, median, mode or any particular value.\n",
    "Sklearn provides Imputer() method to perform imputation in 1 line of code. We just need to define missing_values, axis, and strategy. We are using “median” value of the column to substitute with the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values=\"NaN\", strategy='median', axis=0)\n",
    "X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test data split:\n",
    "\n",
    "For dividing data into train data & test data. We are using train_test_split() method by sklearn.\n",
    "train_test_split(): We are using 4 parameters X, Y, test_size, random_state\n",
    "\n",
    "   * X, Y:  X is a numpy array consisting of feature dataset & Y contains labels for each record.\n",
    "   \n",
    "   * test_size: It represents the size of test data needs to split. If we use 0.4, it indicates 40% of data should be separated and saved as testing data.\n",
    "   \n",
    "   * random_state: It’s pseudo-random number generator state used for random sampling. If you want to replicate our results, then use the same value of random_state.\n",
    "   \n",
    "Now, X_train & y_train are training datasets. X_test & y_test are testing datasets.\n",
    "y_train & y_test are 2d numpy arrays with 1 column. To convert it into a 1d array, we are using ravel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, Y, test_size = 0.3, random_state = 100)\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Implementation:\n",
    "\n",
    "Now we are fitting KNN algorithm on training data, predicting labels for dataset and printing the accuracy of the model for different values of K(ranging from 1 to 25).\n",
    "\n",
    "**KNeighborsClassifier():** This is the classifier function for KNN. It is the main function for implementing the algorithms. Some important parameters are:\n",
    "\n",
    "   * n_neighbors: It holds the value of K, we need to pass and it must be an integer. If we don’t give the value of n_neighbors then by default, it takes the value as 5.\n",
    "   * Weights: It holds a string value i.e., name of the weight function. The Weight function used in prediction. It can hold values like ‘uniform’ or ‘distance’ or any user defined function.\n",
    "      * ‘uniform’ weight used when all points in the neighborhood are weighted equally. Default value for weights taken as ‘uniform’\n",
    "      * ‘distance’ weight used for giving closer neighbors- higher weight and far neighbors-less weight, i.e., weight points by the inverse of their distance.\n",
    "      * user defined function we can call the user defined functions. The user defined function can used when we want to produce custom weight values. It accepts distance values and returns an array of weights.\n",
    "   * algorithm: It specifies algorithm which should be used to compute the nearest neighbors. It can values like ‘auto’, ‘ball_tree’, ‘kd_tree’, brute’. It is an optional parameter.\n",
    "      * a) ‘ball_tree’ , ‘kd_tree’ are used to implement ball tree algorithm. These are special kind of data structures for space partitioning.\n",
    "      * b) ‘brute’ is used to implement brute-force search algorithm.\n",
    "      * c) ‘auto’ is used to give control to the system. By using ‘auto’, it automatically decides the best algorithm according to values of training data.fit()\n",
    "   * data.fit(): A fit method is used to fit the model. It is passed with two parameters:X and Y. For training data fitting on KNN algorithm, this needs to call.\n",
    "      * X: It consists of training data with features.\n",
    "      * Y: It consists of training data with labels.predict(): It predicts class labels for the data provided as its parameters.\n",
    "If an array of features data is entered as parameters, then an array of labels is given as output.\n",
    "\n",
    "#### Accuracy Score:\n",
    "\n",
    "accuracy_score(): This function is used to print accuracy of KNN algorithm. By accuracy, we mean the ratio of the correctly predicted data points to all the predicted data points. Accuracy as a metric helps to understand the effectiveness of our algorithm. It takes 4 parameters.\n",
    "\n",
    "   * y_true,\n",
    "   * y_pred,\n",
    "   * normalize,\n",
    "   * sample_weight.\n",
    "Out of these 4, normalize & sample_weight are optional parameters. The parameter y_true  accepts an array of correct labels and y_pred takes an array of predicted labels that are returned by the classifier. It returns accuracy as a float value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  95.2380952381 % for K-Value: 1\n",
      "Accuracy is  93.3333333333 % for K-Value: 2\n",
      "Accuracy is  95.7142857143 % for K-Value: 3\n",
      "Accuracy is  95.2380952381 % for K-Value: 4\n",
      "Accuracy is  95.7142857143 % for K-Value: 5\n",
      "Accuracy is  94.7619047619 % for K-Value: 6\n",
      "Accuracy is  94.7619047619 % for K-Value: 7\n",
      "Accuracy is  94.2857142857 % for K-Value: 8\n",
      "Accuracy is  94.7619047619 % for K-Value: 9\n",
      "Accuracy is  94.2857142857 % for K-Value: 10\n",
      "Accuracy is  94.2857142857 % for K-Value: 11\n",
      "Accuracy is  94.7619047619 % for K-Value: 12\n",
      "Accuracy is  94.7619047619 % for K-Value: 13\n",
      "Accuracy is  93.8095238095 % for K-Value: 14\n",
      "Accuracy is  93.8095238095 % for K-Value: 15\n",
      "Accuracy is  93.8095238095 % for K-Value: 16\n",
      "Accuracy is  93.8095238095 % for K-Value: 17\n",
      "Accuracy is  93.8095238095 % for K-Value: 18\n",
      "Accuracy is  93.8095238095 % for K-Value: 19\n",
      "Accuracy is  93.8095238095 % for K-Value: 20\n",
      "Accuracy is  93.8095238095 % for K-Value: 21\n",
      "Accuracy is  93.8095238095 % for K-Value: 22\n",
      "Accuracy is  93.8095238095 % for K-Value: 23\n",
      "Accuracy is  93.8095238095 % for K-Value: 24\n",
      "Accuracy is  93.8095238095 % for K-Value: 25\n"
     ]
    }
   ],
   "source": [
    "for K in range(25):\n",
    "    K_value = K+1\n",
    "    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n",
    "    neigh.fit(X_train, y_train) \n",
    "    y_pred = neigh.predict(X_test)\n",
    "    print(\"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\",K_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that we are getting 95.71% accuracy on K = 3, 5. Choosing a large value of K will lead to greater amount of execution time & underfitting. Selecting the small value of K will lead to overfitting. There is no such guaranteed way to find the best value of K. So, to run it quickly we are considering K =3 for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours: Pros and Cons\n",
    "\n",
    "#### Pros:\n",
    "   * Simple to implement\n",
    "   * Flexible to feature / distance choices\n",
    "   * Naturally handles multi-class cases\n",
    "   * Can do well in practice with enough representative data\n",
    "\n",
    "\n",
    "#### Cons:\n",
    "   * Large search problem to find nearest neighbours\n",
    "   * Storage of data\n",
    "   * Must know we have a meaningful distance function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "Here we will learn how to use Naive Bayes Classifier to perform a Multi Class Classification on a data set we are already familiar with: the Iris Data Set. This Section will consist of 7 main parts:\n",
    "\n",
    "   * Note on Notation and Math Terms\n",
    "   * Bayes' Theorem\n",
    "   * Introduction to Naive Bayes\n",
    "   * Naive Bayes Classifier Mathematics Overview\n",
    "   * Constructing a classifier from the probability model\n",
    "   * Gaussian Naive Bayes\n",
    "   * Gaussian Naive Bayes with SciKit Learn\n",
    "    \n",
    "Let's go ahead and begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Notation and Math Terms\n",
    "\n",
    "There are a few more advanced notations and amthematical terms used during the explanation of naive Bayes Classification.\n",
    "You should be familiar with the following:\n",
    "\n",
    "#### Product of Sequence\n",
    "\n",
    "The product of a sequence of terms can be written with the product symbol, which derives from the capital letter Π (Pi) in the Greek alphabet. The meaning of this notation is given by:\n",
    " $$\\prod_{i=1}^4 i = 1\\cdot 2\\cdot 3\\cdot 4,  $$\n",
    "that is\n",
    " $$\\prod_{i=1}^4 i = 24. $$\n",
    " \n",
    "#### Arg Max\n",
    "\n",
    "In mathematics, the argument of the maximum (abbreviated arg max or argmax) is the set of points of the given argument for which the given function attains its maximum value. In contrast to global maximums, which refer to a function's largest outputs, the arg max refers to the inputs which create those maximum outputs.\n",
    "\n",
    "The arg max is defined by\n",
    "\n",
    "$$\\operatorname*{arg\\,max}_x  f(x) := \\{x \\mid \\forall y : f(y) \\le f(x)\\}$$\n",
    "\n",
    "In other words, it is the set of points x for which f(x) attains its largest value. This set may be empty, have one element, or have multiple elements. For example, if f(x) is 1−|x|, then it attains its maximum value of 1 at x = 0 and only there, so\n",
    "\n",
    "$$\\operatorname*{arg\\,max}_x (1-|x|) = \\{0\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "First, for a quick introduction to Bayes' Theorem, check out the Bayes' Theorem Lecture in the statistics appendix portion of this course, in order ot fully understand Naive Bayes, you'll need a complete understanding of the Bayes' Theorem.\n",
    "\n",
    "### Introduction to Naive Bayes\n",
    "\n",
    "Naive Bayes is probably one of the practical machine learning algorithms. Despite its name, it is actually performs very well considering its classification performance. It proves to be quite robust to irrelevant features, which it  ignores. It learns and predicts very fast and it does not require lots of storage. So, why is it then called naive?\n",
    "\n",
    "The naive was added to the account for one assumption that is required for Bayes to work optimally: all features must be independent of each other. In reality, this is usually not the case, however, it still returns very good accuracy in practice even when the independent assumption does not hold.\n",
    "\n",
    "Naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. We will be working with the Iris Flower data set in this lecture.\n",
    "\n",
    "### Naive Bayes Classifier Mathematics Overview\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features. Given a class variable y and a dependent feature vector x<sub>1</sub> through x<sub>n</sub>, Bayes’ theorem states the following relationship:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 \n",
    "Using the naive independence assumption that\n",
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$$\n",
    "\n",
    "for all i, this relationship is simplified to:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                               \n",
    "We now have a relationship between the target and the features using Bayes Theorem along with a Naive Assumption that all features are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a classifier from the probability model\n",
    "\n",
    "So far we have derived the independent feature model, the Naive Bayes probability model. The Naive Bayes classifier combines this model with a *decision rule*, this decision rule will decide which hypothesis is most probable, in our example case this will be which class of flower is most probable.\n",
    "\n",
    "Picking the hypothesis that is most probable  is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label to y as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since P(x<sub>1</sub>, ..., x<sub>n</sub>) is constant given the input, we can use the following classification rule:\n",
    "$$P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),$$\n",
    "\n",
    "and we can use Maximum A Posteriori (MAP) estimation to estimate P(y) and P(x<sub>i</sub> | y); the former is then the relative frequency of class y in the training set.\n",
    "\n",
    "There are different naive Bayes classifiers that differ mainly by the assumptions they make regarding the distribution of P(x<sub>i</sub> | y).\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. Go back to the normal distribution lecture to review the formulas for the Gaussian/Normal Distribution.\n",
    "\n",
    "For example of using the Gaussian Distribution, suppose the training data contain a continuous attribute, x. We first segment the data by the class, and then compute the mean and variance of x in each class. Let  μ<sub>c</sub> be the mean of the values in x associated with class c, and let  σ<sup>2</sup><sub>c</sub> be the variance of the values in x associated with class c. Then, the probability distribution of some value given a class, p(x=v|c), can be computed by plugging v into the equation for a Normal distribution parameterized by μ<sub>c</sub> and  σ<sup>2</sup><sub>c</sub>. That is:\n",
    "\n",
    "$$p(x=v|c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_c}}\\,e^{ -\\frac{(v-\\mu_c)^2}{2\\sigma^2_c} }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to Naive Bayes is making the (rather large) assumption that the presences (or absences) of\n",
    "each data feature are independent of one another, conditional on a data having a certain label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes with SciKit Learn\n",
    "We'll start by importing the usual.\n",
    "\n",
    "Quick note we will actually only use the SciKit Learn Library in this lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our module imports, let's go ahead and import the Iris Data Set. We have previously worked with this dataset, so go ahead and look at Lectures on MultiClass Classification for a complete breakdown on this data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the iris datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Grab features (X) and the Target (Y)\n",
    "X = iris.data\n",
    "\n",
    "Y = iris.target\n",
    "\n",
    "# Show the Built-in Data Description\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already done a general analysis of this data in earlier lectures, let's go ahead and move on to using the Naive Bayes Method to seperate this data set into multiple classes.\n",
    "\n",
    "First we create and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a Naive Bayes model to the data\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, we will continue by seperating into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# Split the data into Trainging and Testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit our model using the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the training model\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict the outcomes from the Testing Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicted outcomes\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Actual Expected Outvomes\n",
    "expected = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can see the metrics for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have about 94.7% accuracy using the Naive Bayes method!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

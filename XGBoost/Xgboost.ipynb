{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Xgboost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine.\n",
    "\n",
    "This makes xgboost at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.\n",
    "\n",
    "Since it is very high in predictive power but relatively slow with implementation, “xgboost” becomes an ideal fit for many competitions. It also has additional features for doing cross validation and finding important variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of boosting <a name='idea-of-boosting' />\n",
    "Let's start with intuitive definition of the concept:\n",
    "> **Boosting** (*Freud and Shapire, 1996*) - algorithm allowing to fit **many** weak classifiers to **reweighted** versions of the training data. Classify final examples by majority voting.\n",
    "\n",
    "When using boosting techinque all instance in dataset are assigned a score that tells *how difficult to classify* they are. In each following iteration the algorithm pays more attention (assign bigger weights) to instances that were wrongly classified previously.\n",
    "\n",
    "<img src='images/boosting.png' alt='boosting' style=\"width: 500px;\"/>\n",
    "\n",
    "In the first iteration all instance weights are equal.\n",
    "\n",
    "Ensemble parameters are optimized in **stagewise way** which means that we are calculating optimal parameters for the next classifier holding fixed what was already calculated. This might sound like a limitation but turns out it's a very resonable way of regularizing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak classifier - why tree? <a name='weak-classifier' />\n",
    "First what is a weak classifier?\n",
    "> **Weak classifier** - an algorithm **slightly better** than random guessing.\n",
    "\n",
    "Every algorithm can be used as a base for boosting techinique, but trees have some nice properties that makes them more suitable candidates.\n",
    "\n",
    "#### Pro's\n",
    "- computational scalability,\n",
    "- handling missing values,\n",
    "- robust to outliers,\n",
    "- does not require feature scalling,\n",
    "- can deal with irrelevant inputs,\n",
    "- interpretable (if small),\n",
    "- can handle mixed predictors (quantitive and qualitative)\n",
    "\n",
    "#### Con's\n",
    "- can't extract linear combination of features\n",
    "- small predictive power (high variance)\n",
    "\n",
    "Boosting techinque can try to reduce the variance by **averaging** many **different** trees (where each one is solving the same problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Common Algorithms<a name='common-algorithms' />\n",
    "\n",
    "In every machine learning model the training objective is a sum of a loss function $L$ and regularization $\\Omega$:\n",
    "\n",
    "$$\n",
    "obj = L + \\Omega\n",
    "$$\n",
    "\n",
    "The loss function controls the predictive power of an algorithm and regularization term controls it's simplicity.\n",
    "\n",
    "#### AdaBoost (Adaptive Boosting)\n",
    "The implementation of boosting technique using decision tress (it's a *meta-estimator* which means you can fit any classifier in). The intuitive recipie is presented below:\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "Assume that the number of training samples is denoted by $N$, and the number of iterations (created trees) is $M$. Notice that possible class outputs are $Y=\\{-1,1\\}$\n",
    "\n",
    "1. Initialize the observation weights $w_i=\\frac{1}{N}$ where $i = 1,2, \\dots, N$\n",
    "2. For $m=1$ to $M$:\n",
    "    - fit a classifier $G_m(x)$ to the training data using weights $w_i$,\n",
    "    - compute $err_m = \\frac{\\sum_{i=1}^{N} w_i I (y_i \\neq G_m(x))}{\\sum_{i=1}^{N}w_i}$,\n",
    "    - compute $\\alpha_m = \\log ((1-err_m)/err_m)$,\n",
    "    - set $w_i \\leftarrow w_i \\cdot \\exp [\\alpha_m \\cdot I (y_i \\neq G_m(x)]$, where $i = 1,2, \\dots, N$\n",
    "3. Output $G_m(x) = sign [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$\n",
    "\n",
    "#### Generalized Boosted Models\n",
    "We can take advantage of the fact that the loss function can be represented with a form suitable for optimalization (due to the stage-wise additivity). This creates a class of general boosting algorithms named simply **generalized boosted model (GBM)**.\n",
    "\n",
    "An example of a GBM is **Gradient Boosted Tree** which uses decision tree as an estimator. It can work with different loss functions (regression, classification, risk modeling etc.), evaluate it's  gradient and approximates it with a simple tree (stage-wisely, that minimizes the overall error).\n",
    "\n",
    "AdaBoost is a special case of Gradient Boosted Tree that uses exponential loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How XGBoost helps <a name='how-xgboost-helps' />\n",
    "The problem with most tree packages is that they don't take regularization issues very seriously - they allow to grow many very similar trees that can be also sometimes quite bushy. \n",
    "\n",
    "GBT tries to approach this problem by adding some regularization parameters. We can:\n",
    "- control tree structure (maximum depth, minimum samples per leaf),\n",
    "- control learning rate (shrinkage),\n",
    "- reduce variance by introducing randomness (stochastic gradient boosting - using random subsamples of instances and features)\n",
    "\n",
    "But it could be improved even further. Enter XGBoost.\n",
    "\n",
    "> **XGBoost** (*extreme gradient boosting*) is a **more regularized** version of Gradient Boosted Trees.\n",
    "\n",
    "It was develop by Tianqi Chen in C++ but also enables interfaces for Python, R, Julia.\n",
    "\n",
    "The main advantages:\n",
    "- good bias-variance (simple-predictive) trade-off \"out of the box\",\n",
    "- great computation speed,\n",
    "- package is evolving (author is willing to accept many PR from community)\n",
    "\n",
    "XGBoost's objective function is a sum of a specific loss function evaluated over all predictions and a sum of regularization term for all predictors ($K$ trees). In the formula $f_k$ means a prediction coming from k-th tree.\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{k=1}^{K} \\Omega (f_k)\n",
    "$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "First part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the their's scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data<a name='data' />\n",
    "We are going to use bundled [Agaricus](https://archive.ics.uci.edu/ml/datasets/Mushroom) dataset which can be downloaded [here](https://github.com/dmlc/xgboost/tree/master/demo/data).\n",
    "\n",
    "> This data set records biological attributes of different mushroom species, and the target is to predict whether it is poisonous\n",
    "\n",
    "> This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom;\n",
    "\n",
    "It consist of 8124 instances, characterized by 22 attributes (both numeric and categorical). The target class is either 0 or 1 which means binary classification problem.\n",
    "\n",
    "> **Important**: XGBoost handles only numeric variables.\n",
    "\n",
    "Lucily all the data have alreay been pre-process for us. Categorical variables have been encoded, and all instances divided into train and test datasets. You will know how to do this on your own in later lectures.\n",
    "\n",
    "Data needs to be stored in `DMatrix` object which is designed to handle sparse datasets. It can be populated in couple ways:\n",
    "- using libsvm format txt file,\n",
    "- using Numpy 2D array (most popular),\n",
    "- using XGBoost binary buffer file\n",
    "\n",
    "In this case we'll use first option.\n",
    "\n",
    "> Libsvm files stores only non-zero elements in format \n",
    "> \n",
    "> `<label> <feature_a>:<value_a> <feature_c>:<value_c> ... <feature_z>:<value_z>`\n",
    ">\n",
    "> Any missing features indicate that it's corresponding value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what was loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 6513 rows and 127 columns\n",
      "Test dataset contains 1611 rows and 127 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset contains {0} rows and {1} columns\".format(dtrain.num_row(), dtrain.num_col()))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".format(dtest.num_row(), dtest.num_col()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train possible labels: \n",
      "[ 0.  1.]\n",
      "\n",
      "Test possible labels: \n",
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train possible labels: \")\n",
    "print(np.unique(dtrain.get_label()))\n",
    "\n",
    "print(\"\\nTest possible labels: \")\n",
    "print(np.unique(dtest.get_label()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training parameters<a name='params' />\n",
    "Let's make the following assuptions and adjust algorithm parameters to it:\n",
    "- we are dealing with binary classification problem (`'objective':'binary:logistic'`),\n",
    "- we want shallow single trees with no more than 2 levels (`'max_depth':2`),\n",
    "- we don't any oupout (`'silent':1`),\n",
    "- we want algorithm to learn fast and aggressively (`'eta':1`),\n",
    "- we want to iterate only 5 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':2,\n",
    "    'silent':1,\n",
    "    'eta':1\n",
    "}\n",
    "\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier<a name='train' />\n",
    "To train the classifier we simply pass to it a training dataset, parameters list and information about number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe performance on test dataset using `watchlist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\n",
      "[1]\ttest-error:0.021726\ttrain-error:0.022263\n",
      "[2]\ttest-error:0.006207\ttrain-error:0.007063\n",
      "[3]\ttest-error:0.018001\ttrain-error:0.0152\n",
      "[4]\ttest-error:0.006207\ttrain-error:0.007063\n"
     ]
    }
   ],
   "source": [
    "watchlist  = [(dtest,'test'), (dtrain,'train')] # native interface only\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions<a name='predict' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08073306,  0.92217326,  0.08073306, ...,  0.98059034,\n",
       "        0.01182149,  0.98059034], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_prob = bst.predict(dtest)\n",
    "preds_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate simple accuracy metric to verify the results. Of course validation should be performed accordingly to the dataset, but in this case accuracy is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted correctly: 1601/1611\n",
      "Error: 0.0062\n"
     ]
    }
   ],
   "source": [
    "labels = dtest.get_label()\n",
    "preds = preds_prob > 0.5 # threshold\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if (labels[i] == preds[i]):\n",
    "        correct += 1\n",
    "\n",
    "print('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\n",
    "print('Error: {0:.4f}'.format(1-correct/len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-learn Interface\n",
    "The following notebook presents the alternative approach for using XGBoost algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries<a name='libs' />\n",
    "Begin with loading all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data<a name='data' />\n",
    "We are going to use the same dataset as in previous lecture. The scikit-learn package provides a convenient function `load_svmlight` capable of reading many libsvm files at once and storing them as Scipy's sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_svmlight_files(('data/agaricus.txt.train', 'data/agaricus.txt.test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine what was loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training parameters<a name='params' />\n",
    "All the parameters are set like in the previous example\n",
    "- we are dealing with binary classification problem (`'objective':'binary:logistic'`),\n",
    "- we want shallow single trees with no more than 2 levels (`'max_depth':2`),\n",
    "- we don't any oupout (`'silent':1`),\n",
    "- we want algorithm to learn fast and aggressively (`'learning_rate':1`), (in naive named `eta`)\n",
    "- we want to iterate only 5 rounds (`n_estimators`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 1.0,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier<a name='train' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = XGBClassifier(**params).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions<a name='predict' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0., ...,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate obtained error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted correctly: 1601/1611\n",
      "Error: 0.0062\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if (y_test[i] == preds[i]):\n",
    "        correct += 1\n",
    "        \n",
    "acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\n",
    "print('Error: {0:.4f}'.format(1-acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify training parameters - we are going to use 5 decision tree stumps with average learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify general training parameters\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':0.5\n",
    "}\n",
    "\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model let's also specify `watchlist` array to observe it's performance on the both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watchlist  = [(dtest,'test'), (dtrain,'train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using predefined evaluation metrics<a name='pmetrics' />\n",
    "\n",
    "#### What is already available?\n",
    "There are already some predefined metrics availabe. You can use them as the input for the `eval_metric` parameter while training the model.\n",
    "\n",
    "- `rmse` - [root mean square error](https://www.wikiwand.com/en/Root-mean-square_deviation),\n",
    "- `mae` - [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error?oldformat=true),\n",
    "- `logloss` - [negative log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function?oldformat=true)\n",
    "- `error` - binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. Treat predicted values with probability $p > 0.5$ as positive,\n",
    "- `merror` - multiclass classification error rate. It is calculated as `#(wrong cases)/#(all cases)`,\n",
    "- `auc` - [area under curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic?oldformat=true),\n",
    "- `ndcg` - [normalized discounted cumulative gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain?oldformat=true),\n",
    "- `map` - [mean average precision](https://en.wikipedia.org/wiki/Information_retrieval?oldformat=true)\n",
    "\n",
    "By default an `error` metric will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "[1]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "[2]\ttest-error:0.03352\ttrain-error:0.030401\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.021495\n",
      "[4]\ttest-error:0.031037\ttrain-error:0.025487\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change is simply specify the `eval_metric` argument to the `params` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttrain-logloss:0.460108\n",
      "[1]\ttest-logloss:0.383911\ttrain-logloss:0.378728\n",
      "[2]\ttest-logloss:0.312678\ttrain-logloss:0.308061\n",
      "[3]\ttest-logloss:0.26912\ttrain-logloss:0.26139\n",
      "[4]\ttest-logloss:0.239746\ttrain-logloss:0.232174\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use multiple evaluation metrics at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttest-auc:0.892138\ttrain-logloss:0.460108\ttrain-auc:0.888997\n",
      "[1]\ttest-logloss:0.383911\ttest-auc:0.938901\ttrain-logloss:0.378728\ttrain-auc:0.942881\n",
      "[2]\ttest-logloss:0.312678\ttest-auc:0.976157\ttrain-logloss:0.308061\ttrain-auc:0.981415\n",
      "[3]\ttest-logloss:0.26912\ttest-auc:0.979685\ttrain-logloss:0.26139\ttrain-auc:0.985158\n",
      "[4]\ttest-logloss:0.239746\ttest-auc:0.9785\ttrain-logloss:0.232174\ttrain-auc:0.983744\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = ['logloss', 'auc']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom evaluation metric<a name='cmetrics' />\n",
    "\n",
    "In order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - predicted probabilities and `DMatrix` object holding training data.\n",
    "\n",
    "In this example our classification metric will simply count the number of misclassified examples assuming that classes with $p> 0.5$ are positive. You can change this threshold if you want more certainty. \n",
    "\n",
    "The algorithm is getting better when the number of misclassified examples is getting lower. Remember to also set the argument `maximize=False` while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom evaluation metric\n",
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label() # obtain true labels\n",
    "    preds = pred_probs > 0.5 # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[1]\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[2]\ttest-misclassified:54\ttrain-misclassified:198\n",
      "[3]\ttest-misclassified:44\ttrain-misclassified:140\n",
      "[4]\ttest-misclassified:50\ttrain-misclassified:166\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, feval=misclassified, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Imbalanced Dataset\n",
    "\n",
    "There are plenty of examples in real-world problems that deals with imbalanced target classes. Imagine medical data where there are only a few positive instances out of thousands of negatie (normal) ones. Another example might be analyzing fraud transaction, in which the actual frauds represent only a fraction of all available data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General advices<a name='generaladvices' />\n",
    "These are some common tactics when approaching imbalanced datasets:\n",
    "\n",
    "- collect more data,\n",
    "- use better evaluation metric (that notices mistakes - ie. AUC, F1, Kappa, ...),\n",
    "- try oversampling minority class or undersampling majority class,\n",
    "- generate artificial samples of minority class (ie. SMOTE algorithm)\n",
    "\n",
    "In XGBoost you can try to:\n",
    "- make sure that parameter `min_child_weight` is small (because leaf nodes can have smaller size groups), it is set to `min_child_weight=1` by default,\n",
    "- assign more weights to specific samples while initalizing `DMatrix`,\n",
    "- control the balance of positive and negative weights  using `set_pos_weight` parameter,\n",
    "- use AUC for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
